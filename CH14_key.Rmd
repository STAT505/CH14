---
title: "CH 14: More Logistic Regression"
output: pdf_document
---

\renewcommand{\vec}[1]{\mathbf{#1}}

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, fig.height = 4, fig.width = 6, fig.align = 'center')
library(tidyverse) 
library(rstanarm)
library(rstantools)
library(arm)
set.seed(11062020)
```


### Odds Ratios

If there are two outcomes, with probabilities $p$ and $1-p$, then $\frac{p}{1-p}$ is called odds.

\vfill

*If the two probabilities are equal then the odds would be $\frac{1/2}{1/2}=1$. If the odds are 2 (or 1/2), this corresponds to p = 2/3 and q = 1/3.*

\vfill

An odds ratio is the result of dividing two odds:

_$$\frac{p_1 / (1 - p_1)}{p_2 / (1 - p_2)}$$._

_an odds ratio of two corresponds to a change in odds, rather than a change in probabilities associated with events 1 and 2._

\vfill

logistic regression can be re-written as

\begin{align}
y & \sim Bernoulli\\
\log \left( \frac{Pr[y = 1|X]}{Pr[y = 0|X]} \right)& = \beta_0 + \beta_1 x \\
\log \left( \frac{Pr[y = 1|X]}{1-Pr[y = 1|X]} \right)& = \beta_0 + \beta_1 x \\
\end{align}

_Thus, a one unit change in $x$ increases the log odds of $y$ by a factor of $\beta_1$_

\vfill
\newpage


Furthermore, logistic regression can also re-written as

\begin{align}
y & \sim Bernoulli\\
\log \left( \frac{Pr[y = 1|X]}{Pr[y = 0|X]} \right)& = \beta_0 + \beta_1 x \\
\frac{Pr[y = 1|X]}{1-Pr[y = 1|X]}& = \exp \left(\beta_0 + \beta_1 x \right)\\
\end{align}

\vfill
_Then consider $\exp{\beta_1}$_
\vfill

\begin{align}
\exp(\beta_1) &= \frac{\exp(\beta_0 + \beta_1 (x + 1))}{\exp(\beta_0 + \beta_1 (x))}\\
&= \frac{Pr[y = 1|X= x + 1]/Pr[y = 0|X= x + 1]}{Pr[y = 1|X= x]/Pr[y = 0|X= x]}
\end{align}

_hence, this can be interpreted as an odds ratio_

\vfill

Interpretation of log odds and odds ratios can be difficult; however, interpreting the impact on probabilities requires setting other parameter values and the change is non-linear (different change in probability for a one unit change in a predictor).

\vfill

\newpage

### Data visualization

```{r}
beer <- read_csv('http://math.montana.edu/ahoegh/Data/Brazil_cerveja.csv') %>% 
  mutate(consumed = consumed - mean(consumed))

bayes_logistic <- stan_glm(weekend ~ consumed, data = beer,
                           family = binomial(link = "logit"), refresh = 0)

beer %>% ggplot(aes(y = weekend, x = consumed)) + 
  geom_point(alpha = .1) + 
  geom_smooth(formula = 'y~x', method = 'loess', color = 'red', se = F) + 
  geom_rug() + ggtitle('Weekend vs. Consumption: comparing glm and loess') + 
  theme_bw() + xlab('Difference in consumption from average daily consumption (L)') +
  geom_line(inherit.aes = F, data = tibble(temp = seq(-15,15, by = .1), 
            y = plogis(coef(bayes_logistic)['(Intercept)'] + coef(bayes_logistic)['consumed']*temp)),
             aes(x=temp, y=y), color = 'blue',lwd = 1) + 
  labs(caption = 'red curve is LOESS fit, blue curve estimated from logistic regression')
```

\newpage

### Model interpretation

```{r}
bayes_logistic
```

\vfill

- (Intercept): _we can interpret this term with all other predictors constant - another good reason to standardize variables. Hence with an average daily consumption, the probability of the day being a weekend is $logit^{-1}(-1.3)=$ `r round(plogis(-1.3),2)`. (with minimal uncertainty)_

\vfill

- consumed: _for each additional unit of consumption, the the log-odds of being a weekend increase by about 0.3 or the odds ratio of being a weekend increases by about $\exp(0.3) =$ `r round(exp(0.3),2)` or the probability of a weekend increases from `r round(plogis(-1.3),2)` to `r round(plogis(-1.3 + .3),2)` if consumption increases from 0 to 1. (with minimal uncertainty)_

\vfill

The last interpretation of the consumed, suggests that scaling variables can also be useful. Then you can state as consumed goes from 0 (the average) to 1 (one standard deviation greater than average) the probability of being a weekend increases from -- to --.

\vfill

\newpage

## Residuals

Just as with standard regression models, _which by the way are a special case of glms, we can use residuals and posterior predictive distributions to evaluate model fit._

\vfill

We can define a residual to be

\begin{align}
r_i &= y_i - Exp[y_i|X_i] \\
&= y_i - logit^{-1}(X_i \beta_i)\\
&= \pi_i
\end{align}


```{r}
binnedplot(predict(bayes_logistic,type = 'response'),resid(bayes_logistic),
           xlab = 'Estimated Probability of Weekend')

binnedplot(beer$consumed,resid(bayes_logistic),
           xlab = 'Difference from average beer consumption (L)')

```

